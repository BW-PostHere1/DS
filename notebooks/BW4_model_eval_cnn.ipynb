{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BW4_model_eval_cnn",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpl3c_OgWpYZ",
        "colab_type": "text"
      },
      "source": [
        "CNN with dropout \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E98Xf35dwiK",
        "colab_type": "text"
      },
      "source": [
        "Process for sequential lstm on text classification \n",
        "1. clean entire corpus, removing stopwords html, and anything else specific to the DS\n",
        "2. split cleaned data into test / train (val)\n",
        "3. tokenize X, fit the transformers on train only then transform test and val\n",
        "4. encode/tokenize y, (class labels) for training * also transform to binary (onehot) MATRIX *\n",
        "5. fit and evaluate model\n",
        "6. use transformers in pipeline to allow predict to consume 'raw input' \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocmyCQSLF2eZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a5debb98-e9c2-4da5-8b49-59c493f24b93"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVjz1umhEY9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "82d57a46-bdb8-4150-fce8-2fe6ff081c66"
      },
      "source": [
        "\n",
        "import csv\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9gQC0zWHE1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "\n",
        "def clean_text(X):\n",
        "    \"\"\" \n",
        "        X: series\n",
        "        \n",
        "        return: np.array\n",
        "    \"\"\"\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    \n",
        "    X = X.map(lambda t : BeautifulSoup(t, \"lxml\").text) # strip html tags\n",
        "    X = X.map(lambda t : t.lower()) # lowercase text\n",
        "    X = X.map(lambda t : REPLACE_BY_SPACE_RE.sub(' ', t))  # symbols by space in text\n",
        "    X = X.map(lambda t : BAD_SYMBOLS_RE.sub('', t)) # delete symbols which are in BAD_SYMBOLS_RE\n",
        "    X = X.map(lambda t : ' '.join(word for word in t.split() if word not in STOPWORDS))# delete stopwords\n",
        "    return np.array(X)\n",
        "\n",
        "text_transformer = FunctionTransformer(clean_text)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Dtm-ApzVZ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sn649JFE53o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/reduced30k.tsv', sep='\\t')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAc0OImEEhBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "newdf  = df.copy()\n",
        "newdf['Text'] =newdf['title'].str.cat(newdf['selftext'], sep=' ')\n",
        "newdf = newdf[['subreddit', 'Text']]\n",
        "newdf.columns = ['Class Name', 'Text']\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkaHcknsFDP6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2621d1a6-0e64-4638-e34e-7172f9a5f6ae"
      },
      "source": [
        "#need convert the labels to numeric\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(newdf['Class Name'])\n",
        "print(integer_encoded, len(label_encoder.classes_))   \n",
        "       \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[164 107 657 ... 802 339 421] 1013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdpuA_guFTN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2a64ec69-458c-4e79-f086-51da388e512d"
      },
      "source": [
        "\n",
        "training_portion = .8\n",
        "train_size = int(newdf.shape[0] * training_portion)\n",
        "\n",
        "train,test = newdf[0: train_size],newdf[train_size:]\n",
        "print(train.shape,test.shape)\n",
        "\n",
        "labels = newdf['Class Name'].tolist()   #use to train label encoder\n",
        "test.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(243120, 2) (60780, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class Name</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>243120</th>\n",
              "      <td>learnmachinelearning</td>\n",
              "      <td>Starting an image recognition network? Hey all...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243121</th>\n",
              "      <td>GenderCritical</td>\n",
              "      <td>Women = infants I’m sure someone has covered t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243122</th>\n",
              "      <td>PlasticSurgery</td>\n",
              "      <td>I am getting a rhinoplasty to fix breathing af...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243123</th>\n",
              "      <td>DebateAltRight</td>\n",
              "      <td>\"Open borders, but only for white countries\" S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243124</th>\n",
              "      <td>pihole</td>\n",
              "      <td>Web Interface (Lighttpd) stops working Some mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Class Name                                               Text\n",
              "243120  learnmachinelearning  Starting an image recognition network? Hey all...\n",
              "243121        GenderCritical  Women = infants I’m sure someone has covered t...\n",
              "243122        PlasticSurgery  I am getting a rhinoplasty to fix breathing af...\n",
              "243123        DebateAltRight  \"Open borders, but only for white countries\" S...\n",
              "243124                pihole  Web Interface (Lighttpd) stops working Some mo..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3l2MXZuHbWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 200\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "labels = newdf['Class Name'].tolist()   #use \n",
        "\n",
        "train_texts = train['Text'].tolist()\n",
        "test_texts = test['Text'].tolist()\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_texts)   #for train, \n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(train_texts)     # sequences is a list of seq\n",
        "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, truncating=trunc_type)  #training X, y\n",
        "\n",
        "### not sure \n",
        "test_sequences = tokenizer.texts_to_sequences(test['Text'].tolist())     #testing X\n",
        "test_padded = pad_sequences(test_sequences,maxlen=max_length)            #X_test_padded\n",
        "\n",
        "\n",
        "train_y = train['Class Name'].tolist()\n",
        "test_y = test['Class Name'].tolist()\n",
        "# transform labels into \n",
        "le  = LabelEncoder()\n",
        "le.fit(newdf['Class Name'])\n",
        "\n",
        "\n",
        "train_y = le.transform(train_y)\n",
        "test_y = le.transform(test_y)\n",
        "train_y = tf.keras.utils.to_categorical(train_y)    # https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n",
        "test_y = tf.keras.utils.to_categorical(test_y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM6Meuv6AU6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5364d3fe-e9b9-4867-e638-ddd0a058990f"
      },
      "source": [
        "\n",
        "len(train_y) , len(X_train_sequences)\n",
        "len(word_index)\n",
        "len(train_texts[0].split())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "267"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13DLRsw2HjCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5c5cbfd-955a-4ca0-984d-2a9a9d5fc94d"
      },
      "source": [
        "%%time \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 13),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1013, activation='softmax')   ## this matches the number of categories\n",
        "    ])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "history = model.fit(X_train_padded,train_y,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(test_padded,test_y),\n",
        "                    callbacks= woahvicky)\n",
        "                    )\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 13)          130000    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 128)         8448      \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1013)              65845     \n",
            "=================================================================\n",
            "Total params: 212,549\n",
            "Trainable params: 212,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 6.3735 - accuracy: 0.0047 - val_loss: 5.8984 - val_accuracy: 0.0120\n",
            "Epoch 2/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 5.5116 - accuracy: 0.0235 - val_loss: 5.0177 - val_accuracy: 0.0545\n",
            "Epoch 3/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 4.7955 - accuracy: 0.0713 - val_loss: 4.4591 - val_accuracy: 0.1256\n",
            "Epoch 4/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 4.3443 - accuracy: 0.1339 - val_loss: 4.1009 - val_accuracy: 0.2020\n",
            "Epoch 5/100\n",
            "7598/7598 [==============================] - 49s 7ms/step - loss: 4.0336 - accuracy: 0.1920 - val_loss: 3.8513 - val_accuracy: 0.2607\n",
            "Epoch 6/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 3.7728 - accuracy: 0.2444 - val_loss: 3.6296 - val_accuracy: 0.3091\n",
            "Epoch 7/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 3.5568 - accuracy: 0.2869 - val_loss: 3.4631 - val_accuracy: 0.3437\n",
            "Epoch 8/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 3.3851 - accuracy: 0.3211 - val_loss: 3.3368 - val_accuracy: 0.3686\n",
            "Epoch 9/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 3.2565 - accuracy: 0.3463 - val_loss: 3.2391 - val_accuracy: 0.3905\n",
            "Epoch 10/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 3.1404 - accuracy: 0.3694 - val_loss: 3.1642 - val_accuracy: 0.4034\n",
            "Epoch 11/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 3.0401 - accuracy: 0.3879 - val_loss: 3.1014 - val_accuracy: 0.4140\n",
            "Epoch 12/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.9478 - accuracy: 0.4067 - val_loss: 3.0481 - val_accuracy: 0.4260\n",
            "Epoch 13/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.8707 - accuracy: 0.4192 - val_loss: 2.9967 - val_accuracy: 0.4338\n",
            "Epoch 14/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.8021 - accuracy: 0.4320 - val_loss: 2.9640 - val_accuracy: 0.4388\n",
            "Epoch 15/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.7484 - accuracy: 0.4408 - val_loss: 2.9335 - val_accuracy: 0.4471\n",
            "Epoch 16/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.7058 - accuracy: 0.4484 - val_loss: 2.9209 - val_accuracy: 0.4518\n",
            "Epoch 17/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.6657 - accuracy: 0.4547 - val_loss: 2.8919 - val_accuracy: 0.4564\n",
            "Epoch 18/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.6332 - accuracy: 0.4613 - val_loss: 2.8886 - val_accuracy: 0.4582\n",
            "Epoch 19/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.5985 - accuracy: 0.4664 - val_loss: 2.8659 - val_accuracy: 0.4600\n",
            "Epoch 20/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.5705 - accuracy: 0.4709 - val_loss: 2.8551 - val_accuracy: 0.4644\n",
            "Epoch 21/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.5475 - accuracy: 0.4747 - val_loss: 2.8418 - val_accuracy: 0.4635\n",
            "Epoch 22/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.5228 - accuracy: 0.4781 - val_loss: 2.8336 - val_accuracy: 0.4667\n",
            "Epoch 23/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.4969 - accuracy: 0.4833 - val_loss: 2.8207 - val_accuracy: 0.4682\n",
            "Epoch 24/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.4740 - accuracy: 0.4862 - val_loss: 2.8295 - val_accuracy: 0.4692\n",
            "Epoch 25/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.4533 - accuracy: 0.4905 - val_loss: 2.8311 - val_accuracy: 0.4714\n",
            "Epoch 26/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.4308 - accuracy: 0.4934 - val_loss: 2.8045 - val_accuracy: 0.4719\n",
            "Epoch 27/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.4121 - accuracy: 0.4957 - val_loss: 2.8046 - val_accuracy: 0.4746\n",
            "Epoch 28/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.3915 - accuracy: 0.4997 - val_loss: 2.8015 - val_accuracy: 0.4766\n",
            "Epoch 29/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.3797 - accuracy: 0.5017 - val_loss: 2.7970 - val_accuracy: 0.4771\n",
            "Epoch 30/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.3625 - accuracy: 0.5038 - val_loss: 2.7999 - val_accuracy: 0.4776\n",
            "Epoch 31/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.3477 - accuracy: 0.5061 - val_loss: 2.7859 - val_accuracy: 0.4781\n",
            "Epoch 32/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.3348 - accuracy: 0.5083 - val_loss: 2.8047 - val_accuracy: 0.4813\n",
            "Epoch 33/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.3224 - accuracy: 0.5103 - val_loss: 2.7906 - val_accuracy: 0.4808\n",
            "Epoch 34/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.3109 - accuracy: 0.5109 - val_loss: 2.7945 - val_accuracy: 0.4820\n",
            "Epoch 35/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.2980 - accuracy: 0.5145 - val_loss: 2.8012 - val_accuracy: 0.4827\n",
            "Epoch 36/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.2860 - accuracy: 0.5151 - val_loss: 2.7831 - val_accuracy: 0.4846\n",
            "Epoch 37/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.2792 - accuracy: 0.5164 - val_loss: 2.7772 - val_accuracy: 0.4831\n",
            "Epoch 38/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.2666 - accuracy: 0.5184 - val_loss: 2.7951 - val_accuracy: 0.4835\n",
            "Epoch 39/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.2616 - accuracy: 0.5188 - val_loss: 2.7899 - val_accuracy: 0.4833\n",
            "Epoch 40/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.2494 - accuracy: 0.5216 - val_loss: 2.8087 - val_accuracy: 0.4844\n",
            "Epoch 41/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.2428 - accuracy: 0.5223 - val_loss: 2.7851 - val_accuracy: 0.4872\n",
            "Epoch 42/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.2334 - accuracy: 0.5229 - val_loss: 2.7891 - val_accuracy: 0.4844\n",
            "Epoch 43/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.2269 - accuracy: 0.5249 - val_loss: 2.7908 - val_accuracy: 0.4859\n",
            "Epoch 44/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.2213 - accuracy: 0.5255 - val_loss: 2.7899 - val_accuracy: 0.4876\n",
            "Epoch 45/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.2130 - accuracy: 0.5267 - val_loss: 2.8061 - val_accuracy: 0.4867\n",
            "Epoch 46/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.2048 - accuracy: 0.5281 - val_loss: 2.8180 - val_accuracy: 0.4862\n",
            "Epoch 47/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1984 - accuracy: 0.5283 - val_loss: 2.8189 - val_accuracy: 0.4852\n",
            "Epoch 48/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.1960 - accuracy: 0.5290 - val_loss: 2.8089 - val_accuracy: 0.4875\n",
            "Epoch 49/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.1879 - accuracy: 0.5292 - val_loss: 2.8103 - val_accuracy: 0.4877\n",
            "Epoch 50/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.1801 - accuracy: 0.5307 - val_loss: 2.8068 - val_accuracy: 0.4893\n",
            "Epoch 51/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.1729 - accuracy: 0.5318 - val_loss: 2.8272 - val_accuracy: 0.4863\n",
            "Epoch 52/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1703 - accuracy: 0.5332 - val_loss: 2.8164 - val_accuracy: 0.4858\n",
            "Epoch 53/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1663 - accuracy: 0.5321 - val_loss: 2.8270 - val_accuracy: 0.4874\n",
            "Epoch 54/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1621 - accuracy: 0.5343 - val_loss: 2.8274 - val_accuracy: 0.4869\n",
            "Epoch 55/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.1564 - accuracy: 0.5353 - val_loss: 2.8209 - val_accuracy: 0.4860\n",
            "Epoch 56/100\n",
            "7598/7598 [==============================] - 53s 7ms/step - loss: 2.1488 - accuracy: 0.5349 - val_loss: 2.8318 - val_accuracy: 0.4887\n",
            "Epoch 57/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1458 - accuracy: 0.5363 - val_loss: 2.8508 - val_accuracy: 0.4885\n",
            "Epoch 58/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1433 - accuracy: 0.5354 - val_loss: 2.8127 - val_accuracy: 0.4893\n",
            "Epoch 59/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1343 - accuracy: 0.5379 - val_loss: 2.8527 - val_accuracy: 0.4891\n",
            "Epoch 60/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.1364 - accuracy: 0.5371 - val_loss: 2.8422 - val_accuracy: 0.4891\n",
            "Epoch 61/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.1276 - accuracy: 0.5390 - val_loss: 2.8302 - val_accuracy: 0.4887\n",
            "Epoch 62/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.1301 - accuracy: 0.5379 - val_loss: 2.8442 - val_accuracy: 0.4871\n",
            "Epoch 63/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1247 - accuracy: 0.5389 - val_loss: 2.8361 - val_accuracy: 0.4886\n",
            "Epoch 64/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1192 - accuracy: 0.5398 - val_loss: 2.8422 - val_accuracy: 0.4891\n",
            "Epoch 65/100\n",
            "7598/7598 [==============================] - 50s 7ms/step - loss: 2.1154 - accuracy: 0.5405 - val_loss: 2.8586 - val_accuracy: 0.4903\n",
            "Epoch 66/100\n",
            "7598/7598 [==============================] - 51s 7ms/step - loss: 2.1145 - accuracy: 0.5395 - val_loss: 2.8506 - val_accuracy: 0.4906\n",
            "Epoch 67/100\n",
            "7598/7598 [==============================] - 52s 7ms/step - loss: 2.1100 - accuracy: 0.5408 - val_loss: 2.8751 - val_accuracy: 0.4871\n",
            "Epoch 68/100\n",
            "6303/7598 [=======================>......] - ETA: 7s - loss: 2.0925 - accuracy: 0.5421Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKfKxJANIFOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\n",
        "    'cnn_model1.h5', overwrite=True, include_optimizer=True, save_format=None,\n",
        "    signatures=None, options=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}