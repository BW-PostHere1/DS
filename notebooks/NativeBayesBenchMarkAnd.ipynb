{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rspct.tsv', 'subreddit_info.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(os.listdir(\"./input\"))\n",
    "\n",
    "# running our benchmark code in this kernel lead to memory errors, so \n",
    "# we do a slightly less memory intensive procedure if this is True, \n",
    "# set this as False if you are running on a computer with a lot of RAM\n",
    "# it should be possible to use less memory in this kernel using generators\n",
    "# rather than storing everything in RAM, but we won't explore that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "rspct_df = pd.read_csv('./input/rspct.tsv', sep='\\t')\n",
    "info_df  = pd.read_csv('./input/subreddit_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_train = rspct_df.sample(frac=.3)\n",
    "reddit_info = info_df.sample(frac=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_info = reddit_info[reddit_info.in_data].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_text(row):\n",
    "        return row['title'] + \" \" + row['selftext']\n",
    "\n",
    "reddit_train['text'] = reddit_train[['title', 'selftext']].apply(join_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_index = int(len(reddit_train) * 0.8)\n",
    "\n",
    "train_df, test_df = reddit_train[:train_split_index], reddit_train[train_split_index:]\n",
    "X_train , X_test  = train_df.text, test_df.text\n",
    "y_train, y_test   = train_df.subreddit, test_df.subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "np.save('y_test', y_test)\n",
    "np.save('y_train', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BackYardChickens', 'ConanExiles', 'devops', ..., 'xxfitness',\n",
       "       'INeedAName', 'benzodiazepines'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/popkdodge/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass k=30000 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5831523527476143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "\n",
    "NUM_FEATURES = 30000 \n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = NUM_FEATURES,\n",
    "                                min_df=5,\n",
    "                                ngram_range=(1,2),\n",
    "                                stop_words=None,\n",
    "                                token_pattern='(?u)\\\\b\\\\w+\\\\b',\n",
    "                            )\n",
    "chi2_selector = SelectKBest(chi2, 30000)\n",
    "nb_model = MultinomialNB(alpha=0.1)\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', tf_idf_vectorizer),\n",
    "    ('chi', chi2_selector),\n",
    "    ('Bayes', nb_model)\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Porsche 911 is the hottest car on the market.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = X_test[:1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text[:1] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = pipe.predict(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Porsche'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipedBayed.joblib']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = 'pipedBayed.joblib'\n",
    "joblib.dump(pipe, filename, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7416711cd382370e78ba5079f465ff1d4c6eac3a"
   },
   "source": [
    "## Basic data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b9cd6088f16683e066378ea9d35785f6c89af44c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6d8knd</td>\n",
       "      <td>talesfromtechsupport</td>\n",
       "      <td>Remember your command line switches...</td>\n",
       "      <td>Hi there,  &lt;lb&gt;The usual. Long time lerker, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58mbft</td>\n",
       "      <td>teenmom</td>\n",
       "      <td>So what was Matt \"addicted\" to?</td>\n",
       "      <td>Did he ever say what his addiction was or is h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ti6re</td>\n",
       "      <td>ringdoorbell</td>\n",
       "      <td>Not door bell, but floodlight mount height.</td>\n",
       "      <td>I know this is a sub for the 'Ring Doorbell' b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77sxto</td>\n",
       "      <td>intel</td>\n",
       "      <td>Worried about my 8700k small fft/data stress r...</td>\n",
       "      <td>Prime95 (regardless of version) and OCCT both,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             subreddit  \\\n",
       "0  6d8knd  talesfromtechsupport   \n",
       "1  58mbft               teenmom   \n",
       "2  8f73s7                Harley   \n",
       "3  6ti6re          ringdoorbell   \n",
       "4  77sxto                 intel   \n",
       "\n",
       "                                               title  \\\n",
       "0             Remember your command line switches...   \n",
       "1                    So what was Matt \"addicted\" to?   \n",
       "2                                     No Club Colors   \n",
       "3        Not door bell, but floodlight mount height.   \n",
       "4  Worried about my 8700k small fft/data stress r...   \n",
       "\n",
       "                                            selftext  \n",
       "0  Hi there,  <lb>The usual. Long time lerker, fi...  \n",
       "1  Did he ever say what his addiction was or is h...  \n",
       "2  Funny story. I went to college in Las Vegas. T...  \n",
       "3  I know this is a sub for the 'Ring Doorbell' b...  \n",
       "4  Prime95 (regardless of version) and OCCT both,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rspct_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d2cfcbdd2f7c470c70c73c4bbddb37adc5427a0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>whatsthatbook</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>theydidthemath</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>calculations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>datarecovery</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>data recovery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>declutter</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>declutter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>productivity</td>\n",
       "      <td>advice/question</td>\n",
       "      <td>discipline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       subreddit       category_1     category_2 category_3  in_data  \\\n",
       "0      0   whatsthatbook  advice/question           book        NaN     True   \n",
       "1     25  theydidthemath  advice/question   calculations        NaN     True   \n",
       "2     26    datarecovery  advice/question  data recovery        NaN     True   \n",
       "3     27       declutter  advice/question      declutter        NaN     True   \n",
       "4     30    productivity  advice/question     discipline        NaN     True   \n",
       "\n",
       "  reason_for_exclusion  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that info_df has information on subreddits that are not in data, \n",
    "# we filter them out here\n",
    "\n",
    "info_df = info_df[info_df.in_data].reset_index()\n",
    "info_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc2ff266075c990c3df99de54dc84cd4566b8e6a"
   },
   "source": [
    "## Naive Bayes benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c486638ded3da5e05e16482214250bc2dec49748"
   },
   "outputs": [],
   "source": [
    "# we join the title and selftext into one field\n",
    "\n",
    "def join_text(row):\n",
    "        return row['title'] + \" \" + row['selftext']\n",
    "\n",
    "rspct_df['text'] = rspct_df[['title', 'selftext']].apply(join_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "43c34a5b2ceb8b2f13cc66eb94e170a49ad30433"
   },
   "outputs": [],
   "source": [
    "# take the last 20% as a test set - N.B data is already randomly shuffled,\n",
    "# and last 20% is a stratified split (equal proportions of subreddits)\n",
    "\n",
    "train_split_index = int(len(rspct_df) * 0.8)\n",
    "\n",
    "train_df, test_df = rspct_df[:train_split_index], rspct_df[train_split_index:]\n",
    "X_train , X_test  = train_df.text, test_df.text\n",
    "y_train, y_test   = train_df.subreddit, test_df.subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "f04a86c4ed629ef2ff1180dd7a4191c130fbb3b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([223,  81, 631, 602, 760])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# label encode y\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "np.save('y_test', y_test)\n",
    "np.save('y_train', y_train)\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MPSelectMiniOwners', 'CreditCards', 'greysanatomy', ...,\n",
       "       'Spanish', 'survivor', 'Snus'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ddcca6c2af541e94133222961a83c11f54208e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this cell will take about 10 minutes to run\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# extract features from text using bag-of-words (single words + bigrams)\n",
    "# use tfidf weighting (helps a little for Naive Bayes in general)\n",
    "# note : you can do better than this by extracting more features, then \n",
    "# doing feature selection, but not enough memory on this kernel!\n",
    "\n",
    "print('this cell will take about 10 minutes to run')\n",
    "\n",
    "NUM_FEATURES = 30000 \n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = NUM_FEATURES,\n",
    "                                min_df=5,\n",
    "                                ngram_range=(1,2),\n",
    "                                stop_words=None,\n",
    "                                token_pattern='(?u)\\\\b\\\\w+\\\\b',\n",
    "                            )\n",
    "\n",
    "X_train = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test  = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# if we have more memory, select top 100000 features and select good features\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, 30000)\n",
    "\n",
    "chi2_selector.fit(X_train, y_train) \n",
    "\n",
    "X_train = chi2_selector.transform(X_train)\n",
    "X_test  = chi2_selector.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d317a155229fa60c6241e7b8d2355fb1cba9d43"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# train a naive bayes model, get predictions\n",
    "\n",
    "nb_model = MultinomialNB(alpha=0.1)\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = nb_model.predict_proba(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode y\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "np.save('y_test', y_test)\n",
    "np.save('y_train', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MPSelectMiniOwners', 'CreditCards', 'greysanatomy', ...,\n",
       "       'Spanish', 'survivor', 'Snus'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/popkdodge/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass k=30000 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5831523527476143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "\n",
    "NUM_FEATURES = 30000 \n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features = NUM_FEATURES,\n",
    "                                min_df=5,\n",
    "                                ngram_range=(1,2),\n",
    "                                stop_words=None,\n",
    "                                token_pattern='(?u)\\\\b\\\\w+\\\\b',\n",
    "                            )\n",
    "chi2_selector = SelectKBest(chi2, 30000)\n",
    "nb_model = MultinomialNB(alpha=0.1)\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', tf_idf_vectorizer),\n",
    "    ('chi', chi2_selector),\n",
    "    ('Bayes', nb_model)\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Porsche 911 is the hottest car on the market.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/popkdodge/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:1042: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_with(key, value)\n",
      "/home/popkdodge/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3343: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "X_test[:1] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = pipe.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Porsche'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d31738daa5d0382761477f16e79d1800ac6f730",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we use precision-at-k metrics to evaluate performance\n",
    "# (https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_K)\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argsort(y_pred, axis=1)\n",
    "    y_pred = y_pred[:, ::-1][:, :k]\n",
    "    arr = [y in s for y, s in zip(y_true, y_pred)]\n",
    "    return np.mean(arr)\n",
    "\n",
    "print('precision@1 =', np.mean(y_test == y_pred))\n",
    "print('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\n",
    "print('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "755686e8e4421d8f4a61819bc3c4afa958294779"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'model.sav'\n",
    "pickle.dump(nb_model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-Trained Model (Gnews)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Remember your command line switches... Hi ther...\n",
       "1         So what was Matt \"addicted\" to? Did he ever sa...\n",
       "2         No Club Colors Funny story. I went to college ...\n",
       "3         Not door bell, but floodlight mount height. I ...\n",
       "4         Worried about my 8700k small fft/data stress r...\n",
       "                                ...                        \n",
       "810395    Best workflow for app integration Hi all!<lb><...\n",
       "810396    4K editing machine problems i upgraded my gpu ...\n",
       "810397    Advice on mixing and editing I recently attend...\n",
       "810398    How to properly control hue lights? When I say...\n",
       "810399    First Yak? Alright so I've never owned a kayak...\n",
       "Name: text, Length: 810400, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810400     100+ Classrooms..how do you engage the student...\n",
       "810401     [MANGA SPOILERS] Someone I read **Chapter 48: ...\n",
       "810402     [21 y.o][Beard tips/first time] I really need ...\n",
       "810403     Gluing the tip back on and durability of the b...\n",
       "810404     Your examples of makeup that's not \"standard\" ...\n",
       "                                 ...                        \n",
       "1012995    Is this months rebirth and dungeon astro's wor...\n",
       "1012996    I might need a Medical leave from grad school ...\n",
       "1012997    Police harassing ethnic minorities in Hong Kon...\n",
       "1012998    SU EECS 2030 and EECS 2021 - need advice Hi, I...\n",
       "1012999    What is the worse wine you ever had? My worst ...\n",
       "Name: text, Length: 202600, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         talesfromtechsupport\n",
       "1                      teenmom\n",
       "2                       Harley\n",
       "3                 ringdoorbell\n",
       "4                        intel\n",
       "                  ...         \n",
       "810395                     git\n",
       "810396                premiere\n",
       "810397             VoiceActing\n",
       "810398              amazonecho\n",
       "810399                Kayaking\n",
       "Name: subreddit, Length: 810400, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/popkdodge/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "890843    Blood in the coop! Need advice please. Went in...\n",
       "255027    building planning questions. So, picking this ...\n",
       "876882    Anyone else experiencing fatigue and anger ove...\n",
       "358340    Please help in conversion of FSN to android UP...\n",
       "312967    Advice on Sizing and fit (Levis 501 STF) Absol...\n",
       "                                ...                        \n",
       "378758    Dog has sporadic \"episodes\" of weakness and co...\n",
       "591737    Beta not eating, lethargic, and now floating v...\n",
       "799158    Heavy lifting without equipment Hey ladies, I ...\n",
       "959988    I need a name for my app concept. Okay so here...\n",
       "929993    My Experience Between Xanax and Clonazolam in ...\n",
       "Name: text, Length: 243120, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
       "array([[ 1.4710238 , -2.3969805 ,  1.8748744 ,  1.6224763 , -3.3003945 ,\n",
       "        -3.2372596 , -2.1706448 ,  1.5361576 ,  2.2106836 , -0.27289772,\n",
       "        -2.1587198 ,  1.7079719 ,  0.5790621 ,  0.30224222, -3.8508694 ,\n",
       "         1.4188178 ,  3.6417365 , -1.3552854 , -2.134932  , -1.3678887 ],\n",
       "       [ 2.037355  , -1.8519831 ,  2.107686  ,  3.1011903 , -2.8023505 ,\n",
       "        -4.159095  , -2.0328512 ,  1.809213  , -0.1158124 ,  0.8989647 ,\n",
       "        -2.7574916 ,  2.621982  , -0.43547934, -0.02817489, -4.741898  ,\n",
       "         2.5105217 ,  4.4980564 , -1.5572418 , -2.9888098 , -0.34141865],\n",
       "       [ 4.4281325 , -3.0655746 ,  2.967873  ,  1.0363597 , -3.4089818 ,\n",
       "        -3.5374804 , -0.5658244 ,  1.3100419 ,  1.6979674 , -1.1891607 ,\n",
       "        -0.3206614 ,  2.191861  , -1.1746538 ,  1.0465789 , -5.000568  ,\n",
       "        -0.26257285,  5.5111012 , -1.3371335 , -2.2992864 , -2.4718976 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[20], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1013, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_3 (KerasLayer)   (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1013)              65845     \n",
      "=================================================================\n",
      "Total params: 48,259,917\n",
      "Trainable params: 69,317\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=1000,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=stop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 290,  323,  463, ...,  166, 1011, 1000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIGGER GNEWS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_4 (KerasLayer)   (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1013)              17221     \n",
      "=================================================================\n",
      "Total params: 48,208,637\n",
      "Trainable params: 18,037\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", output_shape=[50],\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1013, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=1000,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=stop )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiki words 250\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/Wiki-words-250/2\",\n",
    "                           input_shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_5 (KerasLayer)   (None, 250)               252343750 \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 245)               61495     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 245)               60270     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 245)               60270     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1013)              249198    \n",
      "=================================================================\n",
      "Total params: 252,774,983\n",
      "Trainable params: 431,233\n",
      "Non-trainable params: 252,343,750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(Dense(245, activation='relu'))\n",
    "model.add(Dense(245, activation='relu'))\n",
    "model.add(Dense(245, activation='relu'))\n",
    "model.add(Dense(1013, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=1000,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=stop )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impplementing BERT\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    if Dropout_num == 0:\n",
    "        # Without Dropout\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    else:\n",
    "        # With Dropout(Dropout_num), Dropout_num > 0\n",
    "        x = Dropout(Dropout_num)(clf_output)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT from the Tensorflow Hub\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "# Encode the text into tokens, masks, and segment flags\n",
    "train_input = bert_encode(X_train.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(X_test.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BERT model with my tuning\n",
    "model_BERT = build_model(bert_layer, max_len=160)\n",
    "model_BERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model_BERT.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split = valid,\n",
    "    epochs = epochs_num, # recomended 3-5 epochs\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size = batch_size_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
